import os
import logging
import joblib
import string
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from .config import VECTORIZER_PATH, MODELS_CONFIG

vectorizer_instance = None
models_loaded_instances = {}
lemmatizer_instance = None
stop_words_set = None

def load_ml_components():
    global vectorizer_instance, models_loaded_instances, lemmatizer_instance, stop_words_set
    try:
        if not os.path.exists(VECTORIZER_PATH):
            raise FileNotFoundError(f"–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω: {VECTORIZER_PATH}")
        vectorizer_instance = joblib.load(VECTORIZER_PATH)
        logging.info("–í–µ–∫—Ç–æ—Ä–∏–∑–∞—Ç–æ—Ä —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω.")

        for model_id, config_data in MODELS_CONFIG.items():
            if not os.path.exists(config_data["path"]):
                raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ '{model_id}' –Ω–µ –Ω–∞–π–¥–µ–Ω: {config_data['path']}")
            models_loaded_instances[model_id] = joblib.load(config_data["path"])
            logging.info(f"–ú–æ–¥–µ–ª—å '{config_data['name']}' ({model_id}) —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ ML –º–æ–¥–µ–ª–µ–π: {e}", exc_info=True)
        raise

    try:
        lemmatizer_instance = WordNetLemmatizer()
        stop_words_set = set(stopwords.words('english'))
        logging.info("NLTK —Ä–µ—Å—É—Ä—Å—ã —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.")
    except LookupError as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ NLTK —Ä–µ—Å—É—Ä—Å–æ–≤: {e}. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –ø–∞–∫–µ—Ç—ã —Å–∫–∞—á–∞–Ω—ã (–≤ Dockerfile).")
        raise

def preprocess_text(text: str) -> str:
    if lemmatizer_instance is None or stop_words_set is None:
        logging.error("NLTK –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (lemmatizer/stopwords) –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã!")
        return ""
    if not isinstance(text, str):
        logging.warning(f"preprocess_text –ø–æ–ª—É—á–∏–ª –Ω–µ —Å—Ç—Ä–æ–∫—É: {type(text)}. –í–æ–∑–≤—Ä–∞—â–∞—é –ø—É—Å—Ç—É—é —Å—Ç—Ä–æ–∫—É.")
        return ""
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = word_tokenize(text)
    processed_tokens = []
    for word in tokens:
        if word not in stop_words_set and word.isalpha():
            processed_tokens.append(lemmatizer_instance.lemmatize(word))
    return " ".join(processed_tokens)

async def predict_fake_news(news_text: str, model_id: str) -> tuple[str, float | None]:
    if vectorizer_instance is None or not models_loaded_instances:
        logging.error("ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (vectorizer/models) –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã!")
        return "–û—à–∏–±–∫–∞: ML –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ –≥–æ—Ç–æ–≤—ã", None

    if model_id not in models_loaded_instances:
        logging.error(f"–ó–∞–ø—Ä–æ—à–µ–Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –º–æ–¥–µ–ª—å: {model_id}")
        return "–û—à–∏–±–∫–∞: –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞", None
    
    selected_model = models_loaded_instances[model_id]
    model_friendly_name = MODELS_CONFIG[model_id]["name"]

    try:
        preprocessed_text = preprocess_text(news_text)
        if not preprocessed_text.strip():
            return "–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å —Ç–µ–∫—Å—Ç", None

        text_vector = vectorizer_instance.transform([preprocessed_text])
        prediction = selected_model.predict(text_vector)[0]
        
        probability = None
        if hasattr(selected_model, "predict_proba"):
            proba = selected_model.predict_proba(text_vector)[0]
            probability = float(max(proba))

        label = "FAKE ü§•" if prediction == 1 else "REAL ‚úÖ"
        logging.info(f"–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é '{model_friendly_name}': {label}, {probability}")
        return label, probability
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ —Å –º–æ–¥–µ–ª—å—é {model_friendly_name}: {e}", exc_info=True)
        return f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è ({model_friendly_name})", None